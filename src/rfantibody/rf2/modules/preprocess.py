from __future__ import annotations

import torch
import torch.nn.functional as F

from rfantibody.rf2.modules import util
from rfantibody.rf2.modules.util import check_for_nans, to_device, Dotdict
from rfantibody.rf2.network import kinematics
from rfantibody.rf2.network.util_module import XYZConverter


class Preprocess:
    """
    Class for handling conversion of a pose to RF2 inputs.
    """

    def __init__(self, pose_to_input: callable, conf: HydraConfig) -> None:
        """
        pose_to_input: function that takes a Pose object and returns a Dotdict dictionary of the core intermediate values
        """
        self.pose_to_input = pose_to_input
        self.conf = conf
        self.xyz_converter = XYZConverter()

    @to_device
    def __call__(self, pose: "Pose") -> tuple:
        """
        Preprocessing function for RF2

        Inputs:

            Pose object. This is used to generate intermediate inputs.
            This is useful for separating training/inference.

            Inputs (Dotdict) containing:

                seq: torch.tensor [L]
                    integer representation of sequence

                xyz_t: torch.tensor [T, L, 27, 3].

                    NB T=1 or 2 (antibody alone or antibody and target)

                idx: torch.tensor [L]. 200 residue offset between chains

                atom_mask: torch.tensor [T, L, 14]

                xyz_true: torch.tensor [L, 27, 3]

        Outputs:
            RF2 network_inputs:

                msa_masked: torch.tensor [1, 1, L, 48]

                msa_full: torch.tensor [1, 1, L, 25]

                seq: torch.tensor [1,L,22]

                xyz_prev: torch.tensor [L, 27, 3]
                    the first template structure
                    In our case, this is always the target

                idx_pdb: torch.tensor [L]

                t1d: torch.tensor [T, L, 23]
                    22 + 1, added for the hotspot layer

                t2d: torch.tensor [T, L, L, 44]
                    made by xyz_to_t2d function (kinematics.py)

                xyz_t: torch.tensor [T, L, 27, 3]

                    xyz coords of the templates

                alpha_t: torch.tensor [T,L,10*3]

                mask_t: torch.tensor [T, L, 27] atom_mask

                mask_prev: [1, L, 27] atom_mask[0]
        """

        inputs = self.pose_to_input(pose, self.conf)

        check_inputs(inputs)

        T, L = inputs.xyz_t.shape[:2]

        # xyzs
        xyz_t, xyz_true, xyz_prev = get_xyzs(inputs)

        # seq/MSA
        seq = inputs.seq
        msa_masked, msa_full, mask_msa, msa = make_msa(seq, L)

        # t1d
        t1d = make_t1d(pose, seq[None], inputs.hotspots, L, T)

        # t2d
        t2d, mask_t_2d = make_t2d(inputs, xyz_t, pose)

        # alpha_t
        alpha_t = make_alpha_t(
            xyz_t, seq[None], t1d, inputs.mask_t, L, self.xyz_converter
        )

        # same_chain
        same_chain = pose.same_chain

        mask_prev, mask_recycle = make_template_masks(inputs, same_chain)

        network_input = {}
        network_input["msa_latent"] = msa_masked
        network_input["msa_full"] = msa_full
        network_input["seq"] = seq
        network_input["idx"] = inputs.idx
        network_input["t1d"] = t1d
        network_input["t2d"] = t2d
        network_input["xyz_t"] = xyz_t
        network_input["alpha_t"] = alpha_t
        network_input["mask_t"] = mask_t_2d
        network_input["same_chain"] = pose.same_chain

        return (
            network_input,
            xyz_prev,
            mask_recycle,
            xyz_true,
            inputs.mask_true,
            msa,
            mask_msa,
            inputs.unclamp,
            inputs.negative,
            inputs.idx,
        )


def check_inputs(inputs: Dotdict) -> None:
    """
    Checks inputs
    """
    if torch.max(inputs.seq) > 21:
        raise ValueError(f"token in sequence is > 21, max is {torch.max(inputs.seq)}")


@check_for_nans
def get_xyzs(inputs: Dotdict) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Gets xyzs from inputs
    """
    xyz_t = util.get_init_xyz(inputs.xyz_t, inputs.mask_t)
    xyz_true = torch.nan_to_num(inputs.xyz_true)
    xyz_prev = xyz_t[0].clone()
    return xyz_t, xyz_true, xyz_prev


@check_for_nans
def make_msa(
    seq: torch.Tensor, L: int
) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Make msa inputs from single sequence (not true MSA)
    """
    seq_onehot = F.one_hot(seq, num_classes=21)  # [1, L, 21]
    # msa_masked
    msa_masked = torch.zeros((1, L, 48))
    msa_masked[:, :, :21] = seq_onehot
    msa_masked[:, :, 22:43] = seq_onehot

    # msa_full
    msa_full = torch.zeros((1, L, 25))
    msa_full[:, :, :21] = seq_onehot

    # mask_msa
    mask_msa = torch.ones_like(msa_full[:, :, 0])  # (T,L)

    # msa
    msa = seq.clone()

    return msa_masked, msa_full, mask_msa, msa


@check_for_nans
def make_t1d(
    pose: "Pose", seq: torch.Tensor, hotspots: torch.Tensor, L: int, T: int
) -> torch.Tensor:
    """
    Makes a t1d.
        Masks antibody sequence and sets confidence to 0
        Sets confidence to 1 for target sequence
        Adds hotspots
    """
    seq_onehot = F.one_hot(seq, num_classes=21)  # [1, L, 21]
    t1d = torch.zeros((T, L, 23))
    t1d[:, :, :21] = seq_onehot.repeat(T, 1, 1)  # [T, L, 23]

    # Mask antibody part
    t1d[:, pose.antibody_mask, :21] = 0
    t1d[:, pose.antibody_mask, 21] = 1

    # template confidence
    t1d[:, pose.target_mask, 21] = 1
    t1d[:, pose.antibody_mask, 21] = 0

    # add pre-masked hotspots
    t1d[0, :, 22] = hotspots

    return t1d


@check_for_nans
def make_t2d(
    inputs: Dotdict, xyz_t: torch.Tensor, pose: "Pose"
) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Makes t2d, and the corresponding mask_t_2d
    """
    mask_t = inputs.mask_t
    mask_t_2d = mask_t[None, :, :, :3].all(dim=-1)  # (B, T, L)
    mask_t_2d = mask_t_2d[:, :, None] * mask_t_2d[:, :, :, None]  # (B, T, L, L)
    assert mask_t_2d.shape[1] == 1

    mask_t_2d[0, 0, : pose.antibody_length, :] = False
    mask_t_2d[0, 0, :, : pose.antibody_length] = False

    t2d = kinematics.xyz_to_t2d(
        xyz_t.unsqueeze(0), mask_t_2d
    )  # [B,T,L,L,44], this function requires a batch dimension
    t2d = t2d.squeeze(0)  # [T,L,L,44]
    return t2d, mask_t_2d


@check_for_nans
def make_alpha_t(
    xyz_t: torch.Tensor,
    seq: torch.Tensor,
    t1d: torch.Tensor,
    mask_t: torch.Tensor,
    L: int,
    xyz_converter: XYZConverter,
) -> torch.Tensor:
    """
    Generates alpha_t
    """
    ### alpha_t ###
    ###############
    seq_tmp = t1d[..., :-1].argmax(dim=-1).reshape(-1, L)
    alpha, _, alpha_mask, _ = xyz_converter.get_torsions(
        xyz_t.reshape(-1, L, 27, 3), seq_tmp
    )
    alpha_mask = torch.logical_and(alpha_mask, ~torch.isnan(alpha[..., 0]))
    alpha[torch.isnan(alpha)] = 0.0
    alpha = alpha.reshape(-1, L, 10, 2)
    alpha_mask = alpha_mask.reshape(-1, L, 10, 1)
    alpha_t = torch.cat((alpha, alpha_mask), dim=-1).reshape(-1, L, 30)
    return alpha_t


@check_for_nans
def make_template_masks(
    inputs: Dotdict, same_chain: torch.Tensor
) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Generates mask_prev, mask_recycle
    """
    mask_prev = inputs.mask_t[:1].clone()  # (1,L,27)
    mask_recycle = mask_prev[:, :, :3].all(dim=-1)
    mask_recycle = mask_recycle[:, :, None] * mask_recycle[:, None, :]  # (B,L,L)
    mask_recycle = same_chain.float() * mask_recycle.float()
    mask_recycle = mask_recycle.squeeze(0)  # (L,L)
    return mask_prev, mask_recycle


def pose_to_inference_RFinput(pose: "Pose", conf: HydraConfig) -> Dotdict:
    """
    Generates input to RF from a Pose object.
    The idea is that anything config-related will be done here.
    """
    seq = pose.seq
    xyz_true = torch.clone(pose.xyz)
    mask_true = torch.clone(pose.atom_mask)
    xyz_t = torch.clone(pose.xyz)[None]
    mask_t = torch.clone(pose.atom_mask)[None]

    # mask the antibody
    xyz_t[:, pose.antibody_mask] = float("nan")
    mask_t[:, pose.antibody_mask] = False
    # mask target sidechains
    xyz_t[:, pose.target_mask, 3:] = float("nan")
    mask_t[:, pose.target_mask, 3:] = False

    # idx
    idx = make_RF_idx(pose)

    # hotspot masking
    hotspots = util.mask_hotspots(pose, conf.inference.hotspot_show_proportion)

    return Dotdict(
        {
            "seq": seq,
            "xyz_t": xyz_t,
            "mask_t": mask_t,
            "idx": idx,
            "hotspots": hotspots,
            "xyz_true": xyz_true,
            "mask_true": mask_true,
            "negative": False,
            "unclamp": False,
        }
    )


def make_RF_idx(pose) -> torch.Tensor:
    """
    Makes the idx feature to RF, adding a 200 residue offset between chains
    """
    idx = []
    lastidx = 0
    for i, ch in enumerate(pose.chains_present):
        chain_idx = pose.idx[pose.chain_dict[ch]]
        chain_idx -= chain_idx[0].clone()
        if i != 0:
            chain_idx += 200 + lastidx
        lastidx = chain_idx[-1]
        idx.append(chain_idx)

    return torch.cat(idx)
